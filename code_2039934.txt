package org.myorg;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {   //extends is executing inheritance, this class is a child class, parent class is          Mapper, child class is WordCountMapper, blue arguments are input key(number of documents) and value datatypes(text inside the document), blacks ones are output key and value datatypes.(the word and its count)    private final static IntWritable one = new IntWritable(1);//count of each word in document, final means constant, every occurrence is 1            @Override    public void map(LongWritable key, Text value, Context context) //blue are hadoop datatypes            throws IOException, InterruptedException //map is a method of parent class, to override it we provide a class with the same name.    {        String line = value.toString(); //Hadoop datatypes being converted to Java datatype        line = line.replaceAll("&.*?\\w+;", " ")               // HTML entities...                .replaceAll("[^a-zA-Z0-9 ]", " ")         // punctuation...                .replaceAll("\\s+", " ");                         if(line != null && !line.trim().isEmpty())        {        	String[] words = line.split(" ");   // split the text to words            // set each word as key to the key-value pair            for(String word : words)                context.write(new Text(word), one);        }        }        }        package org.myorg;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {    @Override    public void reduce(Text key, Iterable<IntWritable> values, Context context)            throws IOException, InterruptedException //iterable is all ones associated with words    {        int sum = 0;        for (IntWritable value : values)         {            sum += value.get();        }        context.write(key, new IntWritable(sum));    }}package org.myorg;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.fs.FileSystem;public class WordCount //driver class{		public static void main(String[] args) throws Exception //main method     {	//thisgiveserrorwhenaproblem is encountered				        Configuration conf = new Configuration();//constructor,same name as class        if (args.length != 3) //when you’re running the jar file, you should have 3 files,driver class, input path,output folder        {            System.err.println("Usage: WordCount<input path><output path>");            System.exit(-1);        }        Job job;//creating a job,configure parameters for created job        job=Job.getInstance(conf, "Word Count");        job.setJarByClass(WordCount.class);//this is the driver class        FileInputFormat.addInputPath(job, new Path(args[1]));//input path is argument 1, jar filename is argument 0, output folder is argument 2        FileOutputFormat.setOutputPath(job, new Path(args[2]));        job.setMapperClass(WordCountMapper.class); //telling which class is mapper and which one         //is reducer (combiner also exists, here reducer is being used as combiner)        job.setReducerClass(WordCountReducer.class);        job.setCombinerClass(WordCountReducer.class);        job.setOutputKeyClass(Text.class);//what will be the key data type be? Text in Hadoop Is equivalent to String in Java        job.setOutputValueClass(IntWritable.class);//primitive data types are too big for Hadoop to use, hence it has equivalents, IntWritable is equivalent of Int in Java, the output should be the number of times the word occurs in the file/document.        // Delete output if exists//if an output folder/directory already exists, delete it        FileSystem hdfs = FileSystem.get(conf);        Path outputDir = new Path(args[2]);        if (hdfs.exists(outputDir))            hdfs.delete(outputDir, true);        System.exit(job.waitForCompletion(true) ? 0 : 1);//if its waiting, the exit command does not get executed    }}